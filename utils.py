import numpy as np
import random
from gurobipy import Model, GRB, LinExpr
import os
import json
import gurobipy as gp
import time
import concurrent.futures
import utils
import math


# # å…¨å±€ï¼å¤–éƒ¨åˆ—è¡¨ï¼Œç”¨æ¥å­˜æ‰€æœ‰ solve çš„ metrics
all_metrics = []
def make_callback(solve_id, metrics_list):
    """è¿”å›ä¸€ä¸ªå›è°ƒå‡½æ•°ï¼Œè¯¥å›è°ƒä¼šæŠŠå½“å‰ solve çš„æ—¶é—´å’Œ gap å­˜åˆ° metrics_list."""
    info = {"solve_id": solve_id,
            "hit": False,        # æ˜¯å¦å·²ç»è®°å½•è¿‡è¾¾åˆ°1%gap
            "time_at_1pct": None,
            "gap_at_hit": None,
            "time_perf_counter":None,

            # å…³äº1000sæ—¶çš„æ•°æ®
            "hit_1000": False,  # æ˜¯å¦å·²è®°å½•è¿‡ 1000 ç§’çŠ¶æ€
            "obj_at_1000": None,
            "gap_at_1000": None,

            # æ¯ä¸€ç§’çš„æ•°æ®
            "trajectory": [], #  æ¯ä¸€ç§’è®°å½• {"time": ..., "obj": ..., "gap": ...}
            }

    metrics_list.append(info)
    last_recorded_second = {"value": -1}
    start = time.perf_counter()
    def cb(model, where):
        if where == GRB.Callback.MIP and not info["hit"]:
            # now = time.perf_counter()
            # ä¸Šé¢çš„è®¡æ—¶æ–¹å¼æ˜¯é—ç•™é—®é¢˜

            elapsed = model.cbGet(GRB.Callback.RUNTIME)
            best = model.cbGet(GRB.Callback.MIP_OBJBST)
            bound = model.cbGet(GRB.Callback.MIP_OBJBND)

            if abs(best) > 1e-6:
                gap = abs(best - bound) / abs(best)
            else:
                gap = float("inf")
            if gap <= 0.01:
                info["hit"] = True
                now = time.perf_counter()
                info["time_perf_counter"] = now
                info["time_at_1pct"] = now - start
                info["gap_at_hit"]  = gap

            # è®°å½• 1000 ç§’æ—¶çš„ç›®æ ‡å€¼
            if elapsed >= 1000 and not info["hit_1000"]:
                info["hit_1000"] = True
                info["obj_at_1000"] = best
                info["gap_at_1000"] = gap

            #  æ¯æ•´ç§’è®°å½•ä¸€æ¬¡ obj/gap
            current_second = int(elapsed)
            if current_second > last_recorded_second["value"]:
                last_recorded_second["value"] = current_second
                info["trajectory"].append({
                    "time": current_second,
                    "obj": best,
                    "gap": gap
                })
    return cb


def make_callback_new(solve_id: str, metrics_list):
    """è¿”å›ä¸€ä¸ªå›è°ƒå‡½æ•°ï¼Œè¯¥å›è°ƒä¼šæŠŠå½“å‰ solve çš„æ—¶é—´å’Œ gap å­˜åˆ° metrics_list."""
    info = {"solve_id": solve_id,
            "hit": False,  # æ˜¯å¦å·²ç»è®°å½•è¿‡è¾¾åˆ°1%gap
            "time_at_1pct": None,
            "gap_at_hit": None,
            "time_perf_counter": None,

            # å…³äº1000sæ—¶çš„æ•°æ®
            "hit_1000": False,  # æ˜¯å¦å·²è®°å½•è¿‡ 1000 ç§’çŠ¶æ€
            "obj_at_1000": None,
            "gap_at_1000": None,

            # æ¯ä¸€ç§’çš„æ•°æ®
            "trajectory": [],  # æ¯ä¸€ç§’è®°å½• {"time": ..., "obj": ..., "gap": ...}
            }

    metrics_list.append(info)
    # ä½¿ç”¨å­—å…¸æ¥æ¨¡æ‹Ÿå¼•ç”¨ï¼Œç¡®ä¿åœ¨é—­åŒ…ä¸­å¯ä»¥ä¿®æ”¹
    last_recorded_second = {"value": -1}
    start = time.perf_counter()

    def cb(model, where):
        if where == GRB.Callback.MIP:
            # è¿™æ˜¯ä¸€ä¸ªä¼˜åŒ–çš„å›è°ƒå‡½æ•°ï¼Œå®ƒé¿å…åœ¨æ¯æ¬¡è°ƒç”¨æ—¶éƒ½åšå¤§é‡è®¡ç®—

            elapsed = model.cbGet(GRB.Callback.RUNTIME)
            best = model.cbGet(GRB.Callback.MIP_OBJBST)
            bound = model.cbGet(GRB.Callback.MIP_OBJBND)

            # è®¡ç®— Gap
            if abs(best) > 1e-6:
                gap = abs(best - bound) / abs(best)
            else:
                gap = float("inf")

            # --- ç›®æ ‡ A: è®°å½•è¾¾åˆ° 1% Gap çš„æ—¶é—´ ---
            if gap <= 0.01 and not info["hit"]:
                info["hit"] = True
                now = time.perf_counter()
                info["time_perf_counter"] = now
                info["time_at_1pct"] = now - start
                info["gap_at_hit"] = gap

            # --- ç›®æ ‡ B: è®°å½• 1000 ç§’æ—¶çš„çŠ¶æ€ ---
            if elapsed >= 1000 and not info["hit_1000"]:
                info["hit_1000"] = True
                info["obj_at_1000"] = best
                info["gap_at_1000"] = gap

            # --- ç›®æ ‡ C: è®°å½•æ¯ç§’çš„æ±‚è§£è½¨è¿¹ ---
            current_second = int(elapsed)
            if current_second > last_recorded_second["value"]:
                last_recorded_second["value"] = current_second
                info["trajectory"].append({
                    "time": current_second,
                    "obj": best,
                    "gap": gap
                })

    return cb


def get_a_assignmeng_lp():
    # åˆ©ç”¨éšæœºæ•°åˆ›å»ºä¸€ä¸ªæˆæœ¬çŸ©é˜µcost_matrix
    driver_num = job_num = 5
    cost_matrix = np.zeros((driver_num, job_num))
    print("åˆ©ç”¨numpyç”Ÿæˆçš„æˆæœ¬çŸ©é˜µ(å…¨é›¶)ä¸ºï¼š\n", cost_matrix)
    for i in range(driver_num):
        for j in range(job_num):
            random.seed(i * 5 + j)
            cost_matrix[i][j] = round(random.random() * 10 + 5, 0)
    print("åˆ©ç”¨rd.randomç”Ÿæˆçš„æ–°æˆæœ¬çŸ©é˜µä¸ºï¼š\n", cost_matrix)  # np.zeros()ç”Ÿæˆçš„ç±»å‹æ˜¯<class 'numpy.ndarray'>
    print(type(cost_matrix))

    # å»ºæ¨¡å¹¶èµ·å
    model = Model("åˆ†é…é—®é¢˜æ¨¡å‹")

    # å®šä¹‰å†³ç­–å˜é‡åŠç±»å‹
    x = [[[] for i in range(driver_num)] for j in range(job_num)]
    for i in range(driver_num):
        for j in range(job_num):
            x[i][j] = model.addVar(vtype=GRB.BINARY, name='x' + str(i + 1) + str(j + 1))

    # ç›®æ ‡
    obj = LinExpr(0)
    for i in range(driver_num):
        for j in range(job_num):
            obj.addTerms(cost_matrix[i][j], x[i][j])
    model.setObjective(obj, GRB.MINIMIZE)

    # çº¦æŸ
    for i in range(driver_num):
        f = LinExpr(0)  # å®šä¹‰ä¸€ä¸ªçº¿æ€§è¡¨è¾¾å¼å«f
        for j in range(job_num):
            f.addTerms(1, x[i][j])  # ä¸€è¡Œçš„01å˜é‡ä¹‹å’Œä¸º1
        model.addConstr(f == 1, name="row" + str(i + 1))
    for j in range(driver_num):
        f = LinExpr(0)
        for i in range(job_num):
            f.addTerms(1, x[i][j])  # ä¸€åˆ—çš„01å˜é‡ä¹‹å’Œä¸º1

    model.write("test_lp.lp")
def decimal_to_binary_list(n, i):
    """
    å°†åè¿›åˆ¶æ•´æ•° i è½¬æ¢ä¸ºäºŒè¿›åˆ¶åˆ—è¡¨ï¼Œç¡®ä¿åˆ—è¡¨é•¿åº¦ä¸ n-1 çš„äºŒè¿›åˆ¶ä½æ•°ç›¸åŒã€‚

    å‚æ•°:
    n (int): ç”¨äºç¡®å®šäºŒè¿›åˆ¶ä½æ•°çš„ä¸Šé™å€¼ï¼ˆç”Ÿæˆçš„äºŒè¿›åˆ¶ä½æ•°ä¸ n-1 çš„ä½æ•°ç›¸åŒï¼‰
    i (int): éœ€è¦è½¬æ¢çš„åè¿›åˆ¶æ•´æ•°

    è¿”å›:
    list: åŒ…å«äºŒè¿›åˆ¶å­—ç¬¦çš„åˆ—è¡¨ï¼Œé•¿åº¦ä¸ n-1 çš„äºŒè¿›åˆ¶ä½æ•°ç›¸åŒ
    """
    if n <= 0:
        raise ValueError("n å¿…é¡»æ˜¯æ­£æ•´æ•°")

    # è®¡ç®—æ‰€éœ€çš„ä½æ•°ï¼ˆå³ n-1 çš„äºŒè¿›åˆ¶ä½æ•°ï¼‰
    max_bits = len(bin(n - 1)) - 2  # å‡2æ˜¯å› ä¸ºbin()è¿”å›çš„å­—ç¬¦ä¸²å‰ç¼€æ˜¯ '0b'

    # å°† i è½¬æ¢ä¸ºæŒ‡å®šä½æ•°çš„äºŒè¿›åˆ¶å­—ç¬¦ä¸²ï¼Œå¹¶æ‹†åˆ†ä¸ºåˆ—è¡¨
    return [int(c) for c in format(i, f'0{max_bits}b')]

def z_score_normalize(lst):
    if not lst:
        return []
    mean = sum(lst) / len(lst)
    variance = sum((x - mean) ** 2 for x in lst) / len(lst)
    std_dev = variance ** 0.5
    if std_dev == 0:  # å¤„ç†æ‰€æœ‰å…ƒç´ ç›¸åŒçš„æƒ…å†µ
        return [0.0] * len(lst)
    return [(x - mean) / std_dev for x in lst]


def gen_primes(n):
    primes = []
    num = 2
    while len(primes) < n:
        if all(num % p != 0 for p in primes):
            primes.append(num)
        num += 1
    return primes


def get_solving_cache(cache:dict,cache_file:str,directory: str, num_problems: int,Threads:int,time_limit=3600):
    # è·å–ç›®å½•ä¸‹æ‰€æœ‰çš„ .lp æ–‡ä»¶
    lp_files = [f for f in os.listdir(directory) if f.endswith('.lp')]
    lp_files.sort()  # æŒ‰æ–‡ä»¶åæ’åºï¼Œç¡®ä¿é¡ºåºä¸€è‡´

    # é™åˆ¶è¯»å–çš„æ–‡ä»¶æ•°é‡
    lp_files = lp_files[:num_problems]

    # log folder
    log_folder = directory.replace('./instance/', './log/')
    log_folder = log_folder + f"_threads_{Threads}"
    os.makedirs(log_folder,exist_ok=True)

    # ä¾æ¬¡è¯»å–å¹¶æ±‚è§£æ¯ä¸ª .lp æ–‡ä»¶
    for lp_file in lp_files:
        # å¾—åˆ°lpè·¯å¾„
        lp_path = os.path.join(directory, lp_file)
        print(f"Processing {lp_file} cache")

        ## åŸé—®é¢˜æ±‚è§£
        # å¦‚æœç¼“å­˜ä¸­å·²æœ‰ç»“æœï¼Œå°±ç›´æ¥è¯»å–ï¼Œå¦åˆ™æ±‚è§£å¹¶å†™å…¥ç¼“å­˜
        if lp_path in cache:
            pass
        else:
            print("------------there is not cache, solving-------------")
            # è¯»å…¥æ¨¡å‹
            model_orig = gp.read(lp_path)

            # æ—¶é—´ä»è¯»å…¥å¼€å§‹ç®—,æ±‚è§£
            model_orig.setParam("Threads", Threads)
            model_orig.setParam('LogFile', os.path.join(log_folder,f'{lp_file}.log') )
            model_orig.setParam("TimeLimit", time_limit)

            # è®°å½•æ˜¯å¦å·²ç»è¾“å‡ºè¿‡ä¿¡æ¯
            cb = make_callback(lp_file, all_metrics)
            t0 = time.perf_counter()
            model_orig.optimize(cb)
            t1 = time.perf_counter()

            # æœ€ä¼˜è§£
            Vars = model_orig.getVars()
            solution = {var.VarName: var.X for var in Vars}

            # æŒ‡æ ‡
            obj_sense = model_orig.ModelSense
            status_orig = model_orig.Status
            obj_orig = model_orig.ObjVal
            time_orig = t1 - t0
            var_num = model_orig.getAttr("NumVars")
            constr_num = model_orig.getAttr("NumConstrs")
            # å†™å…¥ç¼“å­˜
            cache[lp_path] = {
                'time_limit':time_limit,
                'obj_sense':   obj_sense,
                'status_orig': status_orig,
                'obj_orig':    obj_orig,
                'time_orig':   time_orig,
                'var_num':var_num,
                'constr_num':constr_num,
                'solution':solution,
                'hit_1000':all_metrics[-1]["hit_1000"],
                "obj_at_1000":all_metrics[-1]["obj_at_1000"],
                "gap_at_1000":all_metrics[-1]["gap_at_1000"],
                'gap_at_hit_1pct': all_metrics[-1]['gap_at_hit'],
                'hit_1pct_gap': all_metrics[-1]['hit'],
                'time_at_1pct': all_metrics[-1]['time_at_1pct'],
                'every_second':all_metrics[-1]['trajectory']
            }

            # ä¿å­˜log
            with open(cache_file, 'w') as f:
                json.dump(cache, f, indent=2)

def get_gap_cache(cache,cache_file,lp_dir_path, solve_num,Threads):
    # è·å–ç›®å½•ä¸‹æ‰€æœ‰çš„ .lp æ–‡ä»¶
    lp_files = [f for f in os.listdir(lp_dir_path) if f.endswith('.lp')]
    lp_files.sort()  # æŒ‰æ–‡ä»¶åæ’åºï¼Œç¡®ä¿é¡ºåºä¸€è‡´

    # é™åˆ¶è¯»å–çš„æ–‡ä»¶æ•°é‡
    lp_files = lp_files[:solve_num]

    # ä¾æ¬¡è¯»å–å¹¶æ±‚è§£æ¯ä¸ª .lp æ–‡ä»¶
    for lp_file in lp_files:
        # å¾—åˆ°lpè·¯å¾„
        lp_path = os.path.join(lp_dir_path, lp_file)
        print(f"Processing {lp_file} cache")

        ## åŸé—®é¢˜æ±‚è§£
        # å¦‚æœç¼“å­˜ä¸­å·²æœ‰ç»“æœï¼Œå°±ç›´æ¥è¯»å–ï¼Œå¦åˆ™æ±‚è§£å¹¶å†™å…¥ç¼“å­˜
        if lp_path in cache:
            pass
        else:
            print("------------there is not cache, solving-------------")
            # è¯»å…¥æ¨¡å‹
            model_orig = gp.read(lp_path)

            # æ—¶é—´ä»è¯»å…¥å¼€å§‹ç®—,æ±‚è§£
            model_orig.setParam("Threads", Threads)
            # model_orig.setParam("MIPGap", 1e-2)
            t0 = time.perf_counter()

            # è®°å½•æ˜¯å¦å·²ç»è¾“å‡ºè¿‡ä¿¡æ¯
            cb = make_callback(lp_file, all_metrics)
            model_orig.optimize(cb)
            t1 = time.perf_counter()

            # æŒ‡æ ‡
            obj_sense = model_orig.ModelSense
            status_orig = model_orig.Status
            obj_orig = model_orig.ObjVal
            time_gap1_orig = t1 - t0

            # å†™å…¥ç¼“å­˜
            cache[lp_path] = {
                'obj_sense':   obj_sense,
                'status_orig': status_orig,
                'obj_orig':    obj_orig,
                'time_orig':   time_gap1_orig,
                'gap_at_hit_1pct':all_metrics[-1]['gap_at_hit'],
                'hit':all_metrics[-1]['hit'],
                'time_at_1pct':all_metrics[-1]['time_at_1pct']
            }
            with open(cache_file, 'w') as f:
                json.dump(cache, f, indent=2)

def load_gap_cache(cache_dir, task_name, lp_dir_path, solve_num, Threads):
    os.makedirs(cache_dir,exist_ok=True)
    cache_file = os.path.join(cache_dir,f'{task_name}_solve_gap_cache.json')

    # åŠ è½½ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if os.path.exists(cache_file):
        with open(cache_file, 'r') as f:
            cache = json.load(f)
    else:
        cache = {}

    # utils.get_solving_cache(cache,cache_file,lp_dir_path, solve_num,Threads)
    get_gap_cache(cache,cache_file,lp_dir_path, solve_num,Threads)
    return cache

def load_optimal_cache(cache_file, lp_files_dir, solve_num, Threads=0,time_limit=3600):
    # åŠ è½½ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if os.path.exists(cache_file):
        with open(cache_file, 'r') as f:
            cache = json.load(f)
    else:
        cache = {}

    get_solving_cache(cache,cache_file,lp_files_dir, solve_num,Threads,time_limit)
    return cache


def generate_and_save_feasible_model(lp_path, out_dir,
                                     initial_rhs=1,
                                     initial_frac=0.5,
                                     seed=None):
    """
    1. è¯»å–åŸæ¨¡å‹ï¼›
    2. è¿­ä»£æ·»åŠ éšæœº â‰¥ åˆå§‹RHSçš„æ–°çº¦æŸï¼Œæ•°é‡ä¸ºåŸçº¦æŸæ•° * initial_fracï¼›
       - è‹¥å½“å‰æ–°æ¨¡å‹ä¸å¯è¡Œï¼Œåˆ™å°†çº¦æŸæ•°é‡å‡åŠé‡è¯•ï¼›
       - è‹¥å¯è¡Œï¼Œåˆ™å°†æ¨¡å‹å†™å…¥ out_dir/"new_constr" ä¸‹ï¼Œå¹¶åœ¨æ–‡ä»¶ååŠ ä¸Š "new_constr"ã€‚
    """
    if seed is not None:
        random.seed(seed)
        # è¯»å…¥åŸæ¨¡å‹
    model0 = gp.read(lp_path)
    orig_constrs = model0.getConstrs()
    orig_count = len(orig_constrs) // 5


    # è®¡ç®—åˆå§‹è¦æ·»åŠ çš„çº¦æŸæ•°
    num_new = max(1, int(orig_count * initial_frac))

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    save_dir = out_dir + "_new_constr"
    os.makedirs(save_dir, exist_ok=True)

    iteration = 0
    while num_new >= 1:
        iteration += 1
        # æ·±æ‹·è´åŸæ¨¡å‹
        model = model0.copy()
        all_vars = model.getVars()
        # æ·»åŠ  num_new æ¡éšæœº â‰¥ çº¦æŸ
        for i in range(num_new):
            # éšæœºé€‰å˜é‡ä¸ªæ•° k
            k = random.randint(2, 10)
            vars_in_expr = random.sample(all_vars, k)
            expr = gp.quicksum(vars_in_expr)
            model.addConstr(expr >= initial_rhs, name=f"rand_ge_{iteration}_{i}")
        model.update()

        # åˆ¤æ–­å¯è¡Œæ€§
        model.Params.OutputFlag = 0  # å…³é—­æ±‚è§£å™¨æ—¥å¿—
        model.Params.SolutionLimit = 1
        model.optimize()
        status = model.Status

        if status == gp.GRB.SOLUTION_LIMIT:
            print("å·²æ‰¾åˆ°å¯è¡Œè§£ï¼Œæå‰ç»ˆæ­¢")
            Vars = model.getVars()
            # for var in Vars:
            #     print(var.VarName,"\t",var.X)
            # å¯è¡Œï¼Œåˆ™ä¿å­˜æ¨¡å‹å¹¶ç»“æŸ
            base_name = os.path.splitext(os.path.basename(lp_path))[0]
            save_path = os.path.join(
                save_dir,
                f"{base_name}_new_constr_{num_new}.lp"
            )
            model.write(save_path)
            print(f"[è¿­ä»£{iteration}] å¯è¡Œæ¨¡å‹å·²ä¿å­˜ï¼š{save_path}")
            return save_path
        else:
            # ä¸å¯è¡Œï¼Œçº¦æŸæ•°å‡åŠï¼Œé‡è¯•
            print(f"[è¿­ä»£{iteration}] ä¸å¯è¡Œï¼Œçº¦æŸæ•° {num_new} -> {num_new // 2}")
            num_new //= 2

    raise RuntimeError("æ— æ³•é€šè¿‡éšæœºæ·»åŠ  â‰¥ çº¦æŸè·å¾—å¯è¡Œè§£ï¼›æ‰€æœ‰å°è¯•å‡å¤±è´¥ã€‚")

def get_problem_parameters(json_file_path):
    try:
        with open(json_file_path, 'r') as f:
            problem_parameters = json.load(f)
        print(f"æˆåŠŸä» {json_file_path} åŠ è½½å‚æ•°ã€‚")
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°å‚æ•°æ–‡ä»¶ {json_file_path}ã€‚")
        exit(1)  # æ‰¾ä¸åˆ°æ–‡ä»¶
    except json.JSONDecodeError:
        print(f"é”™è¯¯: {json_file_path} æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ã€‚")
        exit(1)  # JSONæ ¼å¼é”™è¯¯
    return problem_parameters

def get_post_fix(param):
    param_values_str = [str(v) for v in param.values()]
    post_fix = "_".join(param_values_str)
    return post_fix


def aggregate_constr(model_agg,agg_num=None,sample=None):
    # å¯¹äºsampleå‡ºçš„çº¦æŸï¼Œè¦åˆ†ä¸ºå¤§äºç­‰äºã€å°äºç­‰äºå’Œç­‰äº
    # sampleæ˜¯çº¦æŸ
    # todo
    conss = model_agg.getConstrs()

    if sample == None:
        sample = random.sample(conss, min(agg_num, len(conss)))
    if agg_num == None:
        agg_num = 50
        print("using default agg num")

    # ä¹˜å­
    # primes = utils.gen_primes(agg_num)
    # u_list = [math.log(p) for p in primes]

    u_list = [1 for i in range(agg_num)]
    # è®¡ç®—èšåˆçº¦æŸ
    agg_coeffs_leq = {}
    agg_rhs_leq = 0.0
    agg_coeffs_geq = {}
    agg_rhs_geq = 0.0
    agg_coeffs_eq = {}
    agg_rhs_eq = 0.0
    for idx, cons in enumerate(sample):
        u = u_list[idx]
        constr_expr = model_agg.getRow(cons)
        sense = cons.Sense
        for j in range(constr_expr.size()):
            var = constr_expr.getVar(j)
            coef = constr_expr.getCoeff(j)
            if sense == "<":
                agg_coeffs_leq[var.VarName] = agg_coeffs_leq.get(var.VarName, 0.0) + u * coef
            elif sense == ">":
                agg_coeffs_geq[var.VarName] = agg_coeffs_geq.get(var.VarName, 0.0) + u * coef
            elif sense == "=":
                agg_coeffs_eq[var.VarName] = agg_coeffs_eq.get(var.VarName, 0.0) + u * coef
            else:
                raise Exception("unknown constr sense")
        if sense == "<":
            agg_rhs_leq += u * cons.RHS
        elif sense == ">":
            agg_rhs_geq += u * cons.RHS
        elif sense == "=":
            agg_rhs_eq += u * cons.RHS
        else:
            raise Exception("unknown constr sense")
        model_agg.remove(cons)  # åˆ é™¤çº¦æŸ
    model_agg.update()

    # æ„é€ èšåˆçº¦æŸ
    expr_leq = 0
    expr_geq = 0
    expr_eq = 0
    for var_name, coef in agg_coeffs_leq.items():
        var = model_agg.getVarByName(var_name)
        expr_leq += coef * var
    for var_name, coef in agg_coeffs_geq.items():
        var = model_agg.getVarByName(var_name)
        expr_geq += coef * var
    for var_name, coef in agg_coeffs_eq.items():
        var = model_agg.getVarByName(var_name)
        expr_eq += coef * var

    model_agg.addConstr(expr_leq <= agg_rhs_leq, name="agg_constraint_leq")
    model_agg.addConstr(expr_geq >= agg_rhs_geq, name="agg_constraint_geq")
    model_agg.addConstr(expr_eq == agg_rhs_eq, name="agg_constraint_eq")
    model_agg.update()


def solve_single_lp(lp_path: str, lp_file: str, log_folder: str, threads_per_solve: int, time_limit: int):
    """
    åœ¨å•ç‹¬çš„è¿›ç¨‹ä¸­æ±‚è§£ä¸€ä¸ª .lp æ–‡ä»¶ã€‚
    è¿”å›: (lp_path, result_dict) æˆ– (lp_path, None)
    """
    # æ‰“å°è¿›ç¨‹IDï¼Œæ–¹ä¾¿è°ƒè¯•
    print(f"------------ [PID: {os.getpid()}] å¼€å§‹å¤„ç† {lp_file}-------------")
    try:
        # **å…³é”®**: é’ˆå¯¹æœ¬æ¬¡æ±‚è§£ï¼Œåˆ›å»ºä¸€ä¸ªæœ¬åœ°åˆ—è¡¨æ¥æ¥æ”¶æŒ‡æ ‡
        # all_metrics_local: List[Dict[str, Any]] = []
        all_metrics_local = []

        # è¯»å…¥æ¨¡å‹
        # Gurobi çš„è®¸å¯è¯éœ€è¦åœ¨æ¯ä¸ªè¿›ç¨‹ä¸­éƒ½æœ‰æ•ˆ
        model_orig = gp.read(lp_path)

        # å‚æ•°è®¾ç½®
        model_orig.setParam("Threads", threads_per_solve)
        model_orig.setParam('LogFile', os.path.join(log_folder, f'{lp_file}.log'))
        model_orig.setParam("TimeLimit", time_limit)

        # ä¼ å…¥æœ¬åœ°åˆ—è¡¨
        cb = make_callback_new(lp_file, all_metrics_local)
        t0 = time.perf_counter()
        model_orig.optimize(cb)
        t1 = time.perf_counter()

        # æŒ‡æ ‡æå–
        obj_sense = model_orig.ModelSense
        status_orig = model_orig.Status
        obj_orig = model_orig.ObjVal if status_orig == GRB.OPTIMAL or status_orig == GRB.TIME_LIMIT else None
        time_orig = t1 - t0
        var_num = model_orig.getAttr("NumVars")
        constr_num = model_orig.getAttr("NumConstrs")

        # ä»æœ¬åœ°åˆ—è¡¨ä¸­æå–å›è°ƒæŒ‡æ ‡
        if not all_metrics_local:
            # å¦‚æœå›è°ƒæ²¡æœ‰è¿è¡Œæˆ–æ²¡æœ‰æ·»åŠ ä»»ä½•å†…å®¹ï¼ˆä¾‹å¦‚ï¼Œæ¨¡å‹è¯»å–å¤±è´¥ï¼‰
            metrics_data = {
                'hit_1000': None, "obj_at_1000": None, "gap_at_1000": None,
                'gap_at_hit_1pct': None, 'hit_1pct_gap': None,
                'time_at_1pct': None, 'every_second': []
            }
        else:
            last_metric = all_metrics_local[-1]
            # è¿™é‡Œçš„é”®åå¿…é¡»ä¸ä¸»å‡½æ•°ä¸­æœŸæœ›å†™å…¥ç¼“å­˜çš„é”®åå®Œå…¨ä¸€è‡´
            metrics_data = {
                'hit_1000': last_metric.get("hit_1000"),
                "obj_at_1000": last_metric.get("obj_at_1000"),
                "gap_at_1000": last_metric.get("gap_at_1000"),
                'gap_at_hit_1pct': last_metric.get('gap_at_hit'),
                'hit_1pct_gap': last_metric.get('hit'),
                'time_at_1pct': last_metric.get('time_at_1pct'),
                'every_second': last_metric.get('trajectory', [])
            }

        # å‡†å¤‡è¦è¿”å›çš„ç¼“å­˜æ•°æ®
        result_dict = {
            'time_limit': time_limit,
            'obj_sense': obj_sense,
            'status_orig': status_orig,
            'obj_orig': obj_orig,
            'time_orig': time_orig,
            'var_num': var_num,
            'constr_num': constr_num,
            **metrics_data  # åˆå¹¶æŒ‡æ ‡
        }

        print(f"------------ [PID: {os.getpid()}] å®Œæˆ {lp_file}-------------")
        return (lp_path, result_dict)

    except Exception as e:
        # æ•è·ä»»ä½•æ±‚è§£æˆ–Gurobié”™è¯¯
        print(f"!!!!!!!! [PID: {os.getpid()}] æ±‚è§£ {lp_file} å¤±è´¥: {e}")
        return (lp_path, None)


def get_solving_cache_parallel(
        cache: dict,
        cache_file: str,
        directory: str,
        num_problems: int,
        threads_per_solve: int,
        num_parallel_solves: int,  # åŒæ—¶è¿è¡Œçš„è¿›ç¨‹æ•°
        time_limit: int = 3600
) -> dict:
    # 1. è·å–å’Œè¿‡æ»¤æ–‡ä»¶
    lp_files = [f for f in os.listdir(directory) if f.endswith('.lp')]
    lp_files.sort()
    lp_files = lp_files[:num_problems]

    # 2. log folder
    log_folder = directory.replace('./instance/', './log/')
    os.makedirs(log_folder, exist_ok=True)

    # 3. è¯†åˆ«éœ€è¦æ±‚è§£çš„ä»»åŠ¡
    tasks_to_submit = []
    for lp_file in lp_files:
        lp_path = os.path.join(directory, lp_file)
        if lp_path not in cache:
            tasks_to_submit.append((lp_path, lp_file, log_folder, threads_per_solve, time_limit))
        else:
            print(f"Processing {lp_file} cache... å·²åœ¨ç¼“å­˜ä¸­ï¼Œè·³è¿‡ã€‚")

    if not tasks_to_submit:
        print("æ‰€æœ‰æŒ‡å®šçš„é—®é¢˜éƒ½å·²åœ¨ç¼“å­˜ä¸­ã€‚")
        return cache

    print(f"æ€»å…±å‘ç° {len(tasks_to_submit)} ä¸ªæ–°é—®é¢˜éœ€è¦æ±‚è§£ã€‚å°†å¯åŠ¨ {num_parallel_solves} ä¸ªè¿›ç¨‹ã€‚")

    # 4. ä½¿ç”¨ ProcessPoolExecutor å¹¶è¡Œæ‰§è¡Œä»»åŠ¡
    cache_updated = False

    # ProcessPoolExecutor ç®¡ç†è¿›ç¨‹çš„åˆ›å»ºå’Œé”€æ¯
    with concurrent.futures.ProcessPoolExecutor(max_workers=num_parallel_solves) as executor:

        future_to_lp_path = {
            # æäº¤ä»»åŠ¡ï¼Œ solve_single_lp åŠå…¶å‚æ•°ä¼šè¢«åºåˆ—åŒ–å¹¶å‘é€ç»™å­è¿›ç¨‹
            executor.submit(solve_single_lp, *task_args): task_args[0]
            for task_args in tasks_to_submit
        }

        # 5. æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_lp_path):
            lp_path = future_to_lp_path[future]
            try:
                result = future.result()

                # result æ˜¯ (lp_path, result_dict)
                if result and result[1]:
                    returned_lp_path, result_dict = result
                    # **å…³é”®**: åœ¨ä¸»è¿›ç¨‹ä¸­æ›´æ–° cache
                    cache[returned_lp_path] = result_dict
                    cache_updated = True
                    print(f"âœ… ç»“æœå·²ç¼“å­˜: {os.path.basename(returned_lp_path)}")
                else:
                    print(f"âŒ ä»»åŠ¡å¤±è´¥æˆ–æ— ç»“æœï¼Œæœªç¼“å­˜: {os.path.basename(lp_path)}")

            except Exception as exc:
                print(f'ğŸ”´ {os.path.basename(lp_path)} ç”Ÿæˆäº†æ„æ–™ä¹‹å¤–çš„å¼‚å¸¸: {exc}')

    # 6. **é‡è¦**: ç»Ÿä¸€ä¿å­˜ cache æ–‡ä»¶
    if cache_updated:
        print("\næ‰€æœ‰ä»»åŠ¡å®Œæˆã€‚æ­£åœ¨ä¿å­˜æ›´æ–°åçš„ç¼“å­˜åˆ°æ–‡ä»¶...")
        try:
            with open(cache_file, 'w') as f:
                json.dump(cache, f, indent=2)
            print(f"ğŸ‰ ç¼“å­˜å·²æˆåŠŸä¿å­˜åˆ° {cache_file}")
        except Exception as e:
            print(f"!!!!!!!! ä¸¥é‡ï¼šä¿å­˜ç¼“å­˜æ–‡ä»¶ {cache_file} å¤±è´¥: {e}")
    else:
        print("\næ²¡æœ‰æ–°çš„ç»“æœéœ€è¦ä¿å­˜ã€‚")

    return cache


def load_cache(cache_file,task_name) -> dict:
    # os.makedirs(cache_dir,exist_ok=True)
    # cache_file = os.path.join(cache_dir,f'{task_name}_solve_cache.json')
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r') as f:
                return json.load(f)
        except json.JSONDecodeError:
            print(f"è­¦å‘Š: ç¼“å­˜æ–‡ä»¶ {cache_file} æŸåæˆ–ä¸ºç©ºï¼Œå°†åˆ›å»ºæ–°ç¼“å­˜ã€‚")
            return {}
    return {}