import gurobipy as gp
from gurobipy import Model, GRB, LinExpr
import os
# from ecole import make_env
# from ecole.core import Observation
# from ecole.scip import Model
import numpy as np
import math
import time
import random
import pandas as pd
import json
import repair_and_post_solve_func
import utils
import concurrent.futures

# k0 = 100
# k1 = 100
# Delta = 10


def accelerated_solving(cache:dict, directory: str, num_problems: int, agg_num: int, seed:int, problem:str, repair_method:str, agg_model_solve_time, time_limit,neighborhood,Threads):
    # è·å–ç›®å½•ä¸‹æ‰€æœ‰çš„ .lp æ–‡ä»¶
    lp_files = [f for f in os.listdir(directory) if f.endswith('.lp')]
    lp_files.sort()  # æŒ‰æ–‡ä»¶åæ’åºï¼Œç¡®ä¿é¡ºåºä¸€è‡´

    # é™åˆ¶è¯»å–çš„æ–‡ä»¶æ•°é‡
    lp_files = lp_files[:num_problems]

    # ç”Ÿæˆä¹˜å­
    # todo:åœ¨æ­¤å¤„è®¾ç½®ä¹˜å­ï¼Ÿ
    primes = utils.gen_primes(agg_num)
    # u_list = [math.log(p) for p in primes]
    u_list = [1 for p in primes]

    results = []

    # ä¾æ¬¡è¯»å–å¹¶æ±‚è§£æ¯ä¸ª .lp æ–‡ä»¶
    for lp_file in lp_files:
        # å¾—åˆ°lpè·¯å¾„
        lp_path = os.path.join(directory, lp_file)
        print(f"Processing {lp_file}")

        ## åŸé—®é¢˜æ±‚è§£
        # å¦‚æœç¼“å­˜ä¸­å·²æœ‰ç»“æœï¼Œå°±ç›´æ¥è¯»å–ï¼Œå¦åˆ™æŠ¥é”™
        if lp_path in cache:
            print("------------read cache-------------")
            entry = cache[lp_path]
            obj_sense = entry['obj_sense']
            status_orig = entry['status_orig']
            obj_orig = entry['obj_orig']
            time_orig = entry['time_orig']
        else:
            raise Exception("there is not cache")

        ## èšåˆé—®é¢˜æ±‚è§£
        # è¯»å…¥é—®é¢˜
        model_agg = gp.read(lp_path)
        t0 = time.perf_counter()

        # æŒ‡æ ‡
        cons_num_orig = model_agg.NumConstrs

        # èšåˆ
        utils.aggregate_constr(model_agg,agg_num)

        # èšåˆåçº¦æŸæ•°é‡
        cons_num_agg = model_agg.NumConstrs

        ## ä¸€äº›æ±‚è§£çš„å‚æ•° todo
        # è§£æ•°é‡ï¼Œæ—¶é—´é™åˆ¶
        # model_agg.setParam('SolutionLimit', 1)
        print("------------solving agg model-------------")
        model_agg.setParam("Threads",Threads)
        model_agg.setParam("Seed", seed)
        if agg_model_solve_time == -1:
            model_agg.optimize()
        else:
            model_agg.setParam("TimeLimit", agg_model_solve_time)
            model_agg.optimize()
        agg_objval_original = model_agg.ObjVal

        # è·å¾—å˜é‡å€¼
        Vars = model_agg.getVars()
        vaule_dict = {var.VarName: var.X for var in Vars}

        # è¯»å…¥æ–°æ¨¡å‹ï¼Œç”¨äºè§£çš„å¯è¡Œæ€§ä¿®å¤
        repair_model = gp.read(lp_path)
        repair_model.setParam("Threads", Threads)
        repair_model.setParam("Seed", seed+1)
        if repair_method == "naive":
            # ç®€å•çš„å¯å‘å¼ä¿®å¤
            vaule_dict = repair_and_post_solve_func.heuristic_repair(repair_model, vaule_dict)
        elif repair_method == "score":
            # å˜é‡è¯„åˆ†
            vaule_dict = repair_and_post_solve_func.heuristic_repair_with_score(repair_model, vaule_dict)
        elif repair_method == "subproblem":
            vaule_dict = repair_and_post_solve_func.heuristic_repair_subproblem(repair_model, vaule_dict)
        elif repair_method == "lightmilp":
            vaule_dict = repair_and_post_solve_func.heuristic_repair_light_MILP(repair_model, vaule_dict, lp_path)
        else:
            raise Exception("unknown repair_method")


        # å¾—åˆ°å¯è¡Œè§£åï¼Œåå¤„ç†
        repair_and_post_solve_func.PostSolve(repair_model,neighborhood,vaule_dict,lp_file,t0,time_limit)
        t1 = time.perf_counter()

        status_agg = repair_model.Status
        obj_agg = repair_model.ObjVal
        total_time_agg = t1 - t0

        ## gapã€æ—¶é—´çº¦ç®€è®¡ç®—
        if obj_sense == GRB.MINIMIZE:
            print("æœ€å°åŒ–é—®é¢˜")
            primal_gap = (obj_agg - obj_orig) / abs(obj_orig) if obj_orig != 0 else float("inf")
        else:
            print("æœ€å¤§åŒ–é—®é¢˜")
            primal_gap = (obj_orig - obj_agg) / abs(obj_orig) if obj_orig != 0 else float("inf")
        time_reduce = (time_orig - total_time_agg) / time_orig if time_orig > 0 else 0

        print(f"åŸobj:{obj_orig},\t èšåˆåobjï¼š{obj_agg}")
        print(f"åŸæ—¶é—´:{time_orig},\t èšåˆåæ—¶é—´:{total_time_agg}")
        print(f"primal_gap:{primal_gap}")
        print(f"time_reduce:{time_reduce}")
        # ä¿å­˜
        results.append({
            "filename": lp_file,
            "status_orig": status_orig,
            "status_agg": status_agg,
            "obj_orig": obj_orig,
            "obj_agg": obj_agg,
            "primal_gap": primal_gap,
            "original_cons_num": cons_num_orig,
            "after_cons_num": cons_num_agg,
            "cons_reduce_ratio": (cons_num_orig - cons_num_agg) / cons_num_orig,
            "time_orig": time_orig,
            "time_agg": total_time_agg,
            "time_reduce": time_reduce,
            "original_cons": cons_num_orig,
            "after_agg_cons": cons_num_agg,
        })
        # å’ŒåŸå§‹æ—¶é—´å¯¹æ¯”

    df = pd.DataFrame(results)
    return df


def solve_single_problem_pipeline(
        lp_path: str,
        lp_file: str,
        cache: dict,
        agg_num: int,
        seed: int,
        repair_method: str,
        agg_model_solve_time: float,
        time_limit: float,
        threads_per_solve: int,  # Gurobi å†…éƒ¨çº¿ç¨‹æ•°
        k0,
        k1,
        Delta
):
    """
    å•ä¸ªè¿›ç¨‹çš„å·¥ä½œå•å…ƒï¼šå¯¹å•ä¸ª LP æ–‡ä»¶æ‰§è¡Œèšåˆã€æ±‚è§£å’Œä¿®å¤æµç¨‹ã€‚
    è¿”å›å•ä¸ªé—®é¢˜çš„ç»“æœå­—å…¸ï¼Œæˆ–è¿”å› None è¡¨ç¤ºå¤±è´¥ã€‚
    """
    try:
        print(f"[PID:{os.getpid()}] Processing {lp_file} with seed {seed}")

        # ----------------- 1. è¯»å–åŸé—®é¢˜ç¼“å­˜ -----------------
        if lp_path not in cache:
            print(f"[PID:{os.getpid()}] é”™è¯¯: {lp_file} ç¼ºå°‘ç¼“å­˜ï¼Œè·³è¿‡ã€‚")
            return None

        entry = cache[lp_path]
        obj_sense = entry['obj_sense']
        status_orig = entry['status_orig']
        obj_orig = entry['obj_orig']
        time_orig = entry['time_orig']

        # ----------------- 2. èšåˆé—®é¢˜æ±‚è§£ -----------------

        # è¯»å…¥é—®é¢˜ï¼Œå¼€å§‹è®¡æ—¶
        model_agg = gp.read(lp_path)
        t0 = time.perf_counter()

        # æŒ‡æ ‡
        cons_num_orig = model_agg.NumConstrs

        # èšåˆ
        # æ³¨æ„: ä¹˜å­ u_list åœ¨è¿™é‡Œä¸å†éœ€è¦ï¼Œå› ä¸ºå®ƒä»¬åœ¨ aggregate_constr ä¸­ä½¿ç”¨
        utils.aggregate_constr(model_agg, agg_num)
        cons_num_agg = model_agg.NumConstrs

        # æ±‚è§£å‚æ•°è®¾ç½®
        model_agg.setParam("Threads", threads_per_solve)
        model_agg.setParam("Seed", seed)
        if agg_model_solve_time != -1:
            model_agg.setParam("TimeLimit", agg_model_solve_time)

        print(f"[PID:{os.getpid()}] Solving aggregated model...")
        model_agg.optimize()
        agg_objval_original = model_agg.ObjVal if model_agg.Status != GRB.INF_OR_UNBD else None

        # è·å¾—å˜é‡å€¼
        Vars = model_agg.getVars()
        vaule_dict = {var.VarName: var.X for var in Vars}

        # ----------------- 3. å¯è¡Œæ€§ä¿®å¤ -----------------

        repair_model = gp.read(lp_path)
        repair_model.setParam("Threads", threads_per_solve)
        repair_model.setParam("Seed", seed + 1)

        if repair_method == "naive":
            vaule_dict = repair_and_post_solve_func.heuristic_repair(repair_model, vaule_dict)
        elif repair_method == "score":
            vaule_dict = repair_and_post_solve_func.heuristic_repair_with_score(repair_model, vaule_dict)
        elif repair_method == "subproblem":
            vaule_dict = repair_and_post_solve_func.heuristic_repair_subproblem(repair_model, vaule_dict)
        elif repair_method == "lightmilp":
            vaule_dict = repair_and_post_solve_func.heuristic_repair_light_MILP(repair_model, vaule_dict, lp_path)
        else:
            raise Exception("unknown repair_method")

        # ----------------- 4. åå¤„ç†å’Œç»“æœè®°å½• -----------------

        # åå¤„ç†
        repair_and_post_solve_func.PostSolve(repair_model, k0, k1, Delta, vaule_dict, lp_file, t0, time_limit)
        t1 = time.perf_counter()

        status_agg = repair_model.Status
        obj_agg = repair_model.ObjVal if status_agg != GRB.INF_OR_UNBD else None
        total_time_agg = t1 - t0

        # gapã€æ—¶é—´çº¦ç®€è®¡ç®—
        if obj_agg is not None and obj_orig is not None and abs(obj_orig) > 1e-6:
            if obj_sense == GRB.MINIMIZE:
                primal_gap = (obj_agg - obj_orig) / abs(obj_orig)
            else:
                primal_gap = (obj_orig - obj_agg) / abs(obj_orig)
        else:
            primal_gap = float("inf")

        time_reduce = (time_orig - total_time_agg) / time_orig if time_orig > 0 else 0

        # å°è£…ç»“æœ
        return {
            "filename": lp_file,
            "seed": seed,  # æ–°å¢ seedï¼Œæ–¹ä¾¿è¿½è¸ª
            "status_orig": status_orig,
            "status_agg": status_agg,
            "obj_orig": obj_orig,
            "obj_agg": obj_agg,
            "primal_gap": primal_gap,
            "original_cons_num": cons_num_orig,
            "after_cons_num": cons_num_agg,
            "cons_reduce_ratio": (cons_num_orig - cons_num_agg) / cons_num_orig,
            "time_orig": time_orig,
            "time_agg": total_time_agg,
            "time_reduce": time_reduce,
            "original_cons": cons_num_orig,
            "after_agg_cons": cons_num_agg,
        }

    except Exception as e:
        print(f"[PID:{os.getpid()}] æ±‚è§£ {lp_file} å¤±è´¥: {e}")
        return None






def solve_lp_files_gurobi_parallel(
        cache: dict,
        directory: str,
        num_problems: int,
        agg_num: int,
        seed: int,
        problem: str,
        repair_method: str,
        PostSolve: bool,
        agg_model_solve_time: float,
        time_limit: float,
        threads_per_solve: int,  # æ¯ä¸ªå®ä¾‹çš„çº¿ç¨‹æ•°
        num_parallel_solves: int,  # å¹¶è¡Œå®ä¾‹æ•°
        k0,
        k1,
        Delta
) -> pd.DataFrame:
    lp_files = [f for f in os.listdir(directory) if f.endswith('.lp')]
    lp_files.sort()
    lp_files = lp_files[:num_problems]

    print(f"--- Seed {seed}: å‡†å¤‡æ±‚è§£ {len(lp_files)} ä¸ªé—®é¢˜ï¼Œå¹¶è¡Œ {num_parallel_solves} ä¸ªè¿›ç¨‹ ---")

    results = []

    # ä½¿ç”¨ ProcessPoolExecutor è¿›è¡Œå¹¶è¡Œæ±‚è§£
    with concurrent.futures.ProcessPoolExecutor(max_workers=num_parallel_solves) as executor:

        future_to_lp_file = {}

        # æäº¤æ‰€æœ‰ä»»åŠ¡
        for lp_file in lp_files:
            lp_path = os.path.join(directory, lp_file)

            # ä¸ºæ¯ä¸ªé—®é¢˜æäº¤ä¸€ä¸ªä»»åŠ¡
            future = executor.submit(
                solve_single_problem_pipeline,
                lp_path,
                lp_file,
                cache,
                agg_num,
                seed,
                repair_method,
                agg_model_solve_time,
                time_limit,
                threads_per_solve,
                k0, k1, Delta
            )
            future_to_lp_file[future] = lp_file

        # æ”¶é›†ç»“æœ
        for future in concurrent.futures.as_completed(future_to_lp_file):
            lp_file = future_to_lp_file[future]
            try:
                result_dict = future.result()
                if result_dict:
                    results.append(result_dict)
                    print(f"âœ… å®Œæˆå¹¶æ”¶é›†ç»“æœ: {lp_file}")
                else:
                    print(f"âŒ {lp_file} æ±‚è§£å¤±è´¥æˆ–è·³è¿‡ã€‚")
            except Exception as exc:
                print(f'ğŸ”´ {lp_file} è¿›ç¨‹äº§ç”Ÿå¼‚å¸¸: {exc}')

    # 4. è½¬æ¢ä¸º DataFrame å¹¶è¿”å›
    df = pd.DataFrame(results)
    return df




# å‡è®¾è¿™ä¸ªå‡½æ•°åŒ…å«æ‚¨çš„é¡¶å±‚å®éªŒé€»è¾‘
def run_all_experiments(seed_list, **kwargs):
    result_dir = kwargs.pop('result_dir')
    agg_num = kwargs['agg_num']
    repair_method = kwargs['repair_method']

    all_runs = []

    # ----------------------------------------------------
    # æ‚¨ç°åœ¨æœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š
    # A. ä¸²è¡Œè¿è¡Œä¸åŒ seed çš„å®éªŒï¼Œä½†æ¯ä¸ª seed å†…éƒ¨æ˜¯å¹¶è¡Œçš„ (æ¨è)
    for seed in seed_list:
        print(f"\n======== å¼€å§‹è¿è¡Œ Seed {seed} å®éªŒ ========")

        # è°ƒç”¨æ–°çš„å¹¶è¡Œå‡½æ•°
        df = solve_lp_files_gurobi_parallel(seed=seed, **kwargs)

        filename = result_dir + f"/random_aggnum_{agg_num}_seed_{seed}_repair_{repair_method}.csv"
        df.to_csv(filename, index=False)
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
        all_runs.append(df)

    # B. (å¦‚æœéœ€è¦) è¿›ä¸€æ­¥å¹¶è¡ŒåŒ–é¡¶å±‚çš„ seed å¾ªç¯ï¼Œä½†éœ€è°¨æ…é¿å… CPU è¿‡è½½ã€‚
    # ----------------------------------------------------

    return all_runs





if __name__ == '__main__':

    problem = "CA"
    json_file_path = 'parameters/param_3.json'
    params = utils.get_problem_parameters(json_file_path)
    post_fix = utils.get_post_fix(params[problem])

    data_dir = "_".join([problem,post_fix])
    dataset_name = "test"
    lp_files_dir = f"./instance/{dataset_name}/{data_dir}"
    seed_list = [2,3,4,5,6]
    solve_num = 10
    agg_num = 50

    Threads = 0 # default 0
    # Threads = 2

    # agg_model_solve_time = -1 # ä¸é™åˆ¶æ—¶é—´
    agg_model_solve_time = 2
    time_limit = 1000
    neighborhood = {"k0":100,"k1":100,"Delta":5}

    # repair_method_list = ["None", "gurobi", "naive", "score", "subproblem","lightmilp"]
    repair_method_list = ["subproblem"]

    for repair_method in repair_method_list:

        result_dir = f"./result/random_{data_dir}_solve_{solve_num}_aggNum_{agg_num}_aggsolvetime_{agg_model_solve_time}_repair_{repair_method}_Threads_{Threads}"

        # ä¿å­˜ç»Ÿè®¡ç»“æœ
        all_runs = []
        os.makedirs(result_dir, exist_ok=True)

        # è¯»å–cache
        cache_dir = f"./cache/{dataset_name}"
        os.makedirs(cache_dir, exist_ok=True)
        cache_file = os.path.join(cache_dir, f'{data_dir}_solving_cache_threads_{Threads}.json')

        ## å•è¿›ç¨‹æ±‚è§£
        cache = utils.load_optimal_cache(cache_file, data_dir, lp_files_dir, solve_num, Threads,time_limit)
        # æ¯ä¸ªseedï¼Œæ±‚è§£ä¸€æ¬¡
        for seed in seed_list:
            random.seed(seed)
            df = accelerated_solving(cache, lp_files_dir, solve_num, agg_num, seed, problem, repair_method, agg_model_solve_time, time_limit,neighborhood,Threads)
            df.to_csv(result_dir + f"/random_aggnum_{agg_num}_seed_{seed}_repair_{repair_method}.csv", index=False)
            all_runs.append(df)

        ## å¤šè¿›ç¨‹æ±‚è§£
        """
        existing_cache = utils.load_cache(cache_file,data_dir)
        final_cache = utils.get_solving_cache_parallel(
            cache=existing_cache,
            cache_file=cache_file,
            directory=lp_files_dir,
            num_problems=solve_num,
            threads_per_solve=Threads,
            num_parallel_solves=10,
            time_limit=time_limit
        )

        # é…ç½®å®éªŒå‚æ•°
        experiment_params = {
            'cache': final_cache,
            'directory': lp_files_dir,
            'num_problems': solve_num,  # ä»…è¿è¡Œä¸€ä¸ªæ¨¡æ‹Ÿé—®é¢˜
            'agg_num': agg_num,
            'problem': 'test_set',
            'repair_method': repair_method,
            'PostSolve': True,
            'agg_model_solve_time': agg_model_solve_time,
            'time_limit': time_limit,
            # æ–°å¢çš„å¹¶è¡Œå’Œçº¿ç¨‹å‚æ•°
            'threads_per_solve': 2,  # æ¯ä¸ªå®ä¾‹ Gurobi ä½¿ç”¨ 2 ä¸ªçº¿ç¨‹
            'num_parallel_solves': 10,  # åŒæ—¶æ±‚è§£ 10 ä¸ªå®ä¾‹
            # ä¿®å¤å’Œåå¤„ç†çš„ç¼ºå¤±å‚æ•°
            'k0': 10,
            'k1': 10,
            'Delta': 5,
            'result_dir': result_dir
        }
        
        # è¿è¡Œå®éªŒ
        all_results = run_all_experiments(seed_list, **experiment_params)
        all_runs = all_results
        """

        # åˆå¹¶æ‰€æœ‰ seed çš„æ˜ç»†
        df_all = pd.concat(all_runs, ignore_index=True)
        df_all.to_csv(os.path.join(result_dir, "all_details.csv"), index=False)

        # æŒ‰ filename æ±‡æ€»ç»Ÿè®¡
        if repair_method != "None":
            # ä¿®å¤ï¼Œprimal gap
            summary = df_all.groupby("filename").agg({
                "primal_gap": ["min", "max", "mean"],
                "time_reduce": ["min", "max", "mean"],
                "cons_reduce_ratio": ["mean"],
                "time_orig": ["mean"],
                "time_agg": ["mean"],
                "obj_orig": ["mean"],
                "obj_agg": ["mean"]
            })
        else:
            # ä¸ä¿®å¤ï¼Œdual gap
            summary = df_all.groupby("filename").agg({
                "dual_gap":    ["min","max","mean"],
                "time_reduce": ["min","max","mean"],
                "cons_reduce_ratio": ["min","max","mean"],
                "time_orig": ["mean"],
                "time_agg": ["mean"],
                "obj_orig": ["mean"],
                "obj_agg": ["mean"]
            })
        # æ‰å¹³åŒ–å¤šçº§åˆ—å
        summary.columns = ["_".join(col).strip() for col in summary.columns.values]
        summary.reset_index(inplace=True)
        summary.to_csv(os.path.join(result_dir, "surrogate_summary.csv"), index=False)

        print("æ˜ç»†å’Œæ‘˜è¦å·²ä¿å­˜è‡³ï¼š", result_dir)


